{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fffd930-6612-41d9-8dfe-e9d0dfc667aa",
   "metadata": {},
   "source": [
    "## Sagemaker Pipeline\n",
    "\n",
    "- An Amazon SageMaker pipeline is a series of interconnected steps in directed acyclic graph (DAG) that are defined using the drag-and-drop UI. Before jumping into CICD we need to understand how pipelins works in Sagemaker as well as How you can build your own pipeline in different usecases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28a2de2-bfc4-4f16-a7dd-a3d4ee644b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "#necessary imports\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.workflow.parameters import ParameterInteger,ParameterString,ParameterFloat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70bf4787-a773-41af-ae09-c836b1d18a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region name ap-south-1\n"
     ]
    }
   ],
   "source": [
    "#defining sagemaker session and getting the region on which we are going to work on\n",
    "sagemaker_session=sagemaker.session.Session()\n",
    "\n",
    "region=sagemaker_session.boto_region_name\n",
    "print(\"region name\",region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28866dea-b71d-47cc-a129-8e8d95be95d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role is arn:aws:iam::730335253621:role/service-role/AmazonSageMaker-ExecutionRole-20241117T225496\n"
     ]
    }
   ],
   "source": [
    "#getting the execution role of my sagemaker\n",
    "role=sagemaker.get_execution_role()\n",
    "print(\"role is\",role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dda2e0c-9fd6-45fa-a45e-77beddc87927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the bucket name and the pipeline model group name\n",
    "bucket_name=\"pipeline-bucket-777/california-pipeline-case-study\"\n",
    "model_package_group_name=\"sagemakerppipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd1355-d939-418f-8b78-50aaecea7da0",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "- Collecting the data from  my s3 bucket which We will use in my further implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7f9675e-575c-410a-9315-7d6f395bd856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://pipeline-bucket-777/california-pipeline-case-study/customer_data.csv\n",
      "s3://pipeline-bucket-777/california-pipeline-case-study/sales_data.csv\n",
      "s3://pipeline-bucket-777/california-pipeline-case-study/shopping_mall_data.csv\n"
     ]
    }
   ],
   "source": [
    "#defining the path of my s3 bucket and uploading it to s3 bucket\n",
    "s3_upload_data_path = f\"s3://{bucket_name}/\"\n",
    "input_customer_data=s3_upload_data_path+\"customer_data.csv\"\n",
    "input_sales_data=s3_upload_data_path+\"sales_data.csv\"\n",
    "input_shopping_mall_data=s3_upload_data_path+\"shopping_mall_data.csv\"\n",
    "\n",
    "print(input_customer_data)\n",
    "print(input_sales_data)\n",
    "print(input_shopping_mall_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a39a52-3e5d-463c-b03e-f7455722f3bb",
   "metadata": {},
   "source": [
    "### Define Parameters to Parametrize Pipeline Execution\n",
    "Define Pipeline parameters that you can use to parametrize the pipeline. \n",
    "\n",
    "Parameters enable custom pipeline executions and schedules without having to modify the Pipeline definition.The supported parameter types include:\n",
    "- ParameterString - represents a str Python type\n",
    "\n",
    "- ParameterInteger - represents an int Python type\n",
    "\n",
    "- ParameterFloat - represents a float Python type\n",
    "\n",
    "These parameters support providing a default value, which can be overridden on pipeline execution.\n",
    "\n",
    "The default value specified should be an instance of the type of the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17dace3d-8907-469b-98ab-60a825d75805",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_instance_count=ParameterInteger(\"ProcessingInstanceCount\",default_value=1)\n",
    "input_data_customer=ParameterString(\"InputDatacustomer\",default_value=input_customer_data)\n",
    "input_data_sales=ParameterString(\"InputDatasales\",default_value=input_sales_data)\n",
    "input_data_shopping_mall=ParameterString(\"InputDatasmall\",default_value=input_shopping_mall_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78636699-14ce-4dd7-8699-bc53be524857",
   "metadata": {},
   "source": [
    "### Define a Processing Step for Feature EngineeringÔÉÅ\n",
    "First, develop a preprocessing script that is specified in the Processing step.This notebook cell writes a file preprocessing_file.py, which contains the preprocessing script. You can update the script, and rerun this cell to overwrite. The preprocessing script uses scikit-learn to do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a18ee48-949b-4f34-9564-a92ec411c54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/preprocessing_file.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/preprocessing_file.py\n",
    "\"\"\"Feature engineers the dataset.\"\"\"\n",
    "\n",
    "# importing all necessary libraries\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import requests\n",
    "import tempfile\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# defining logging here.\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "# defining different encoders for category based columns\n",
    "def category_encoder(input_data):\n",
    "    return input_data.replace(['Clothing', 'Shoes', 'Books', 'Cosmetics', 'Food & Beverage',\n",
    "       'Toys', 'Technology', 'Souvenir'], [0, 1, 2, 3, 4,5,6,7])\n",
    "\n",
    "\n",
    "def mall_encoder(input_data):\n",
    "    return input_data.replace(['South Coast Plaza', 'Beverly Center', 'Westfield Century City',\n",
    "       'Stanford Shopping Center', 'Westfield Valley Fair',\n",
    "       'Del Amo Fashion Center', 'The Grove', 'Glendale Galleria',\n",
    "       'Fashion Valley'], [0, 1, 2, 3, 4,5,6,7,8])\n",
    "\n",
    "\n",
    "def gender_encoder(input_data):\n",
    "    return input_data.replace([\"Male\",\"Female\"], [0, 1])\n",
    "\n",
    "def paymethod_encoder(input_data):\n",
    "    return input_data.replace(['Credit Card', 'Debit Card', 'Cash'], [0, 1, 2])\n",
    "\n",
    "def location_encoder(input_data):\n",
    "    return input_data.replace(['Costa Mesa', 'Los Angeles', 'Palo Alto', 'Santa Clara',\n",
    "       'Torrance', 'Glendale', 'San Diego'], [0, 1, 2,3,4,5,6])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.debug(\"Starting preprocessing.\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument(\"--input-customer-data\", type=str, required=True)\n",
    "    parser.add_argument(\"--input-sales-data\", type=str, required=True)\n",
    "    parser.add_argument(\"--input-mall-data\", type=str, required=True)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # path of the environment variable\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "    \n",
    "    pathlib.Path(f\"{base_dir}/data\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    input_customer_data = args.input_customer_data\n",
    "    input_sales_data = args.input_sales_data\n",
    "    input_data_shopping_mall = args.input_mall_data\n",
    "    # getting the bucket name\n",
    "    bucket = input_sales_data.split(\"/\")[2]\n",
    "    # getting the key name for customer data\n",
    "    key = \"/\".join(input_customer_data.split(\"/\")[3:])\n",
    "\n",
    "    logger.info(\"Downloading data from key: %s\",  key)\n",
    "\n",
    "    # path of the csv file we did uploaded in container\n",
    "    customer_fn = f\"{base_dir}/data/customer_data.csv\"\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    s3.Bucket(bucket).download_file(key, customer_fn)\n",
    "\n",
    "\n",
    "    # getting the key name for sales data\n",
    "    key = \"/\".join(input_sales_data.split(\"/\")[3:])\n",
    "\n",
    "    logger.info(\"Downloading data from key: %s\",  key)\n",
    "\n",
    "    # path of the csv file we did uploaded in container\n",
    "    sales_fn = f\"{base_dir}/data/sales_data.csv\"\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    s3.Bucket(bucket).download_file(key, sales_fn)\n",
    "\n",
    "    # getting the key name for shopping mall data\n",
    "    key = \"/\".join(input_data_shopping_mall.split(\"/\")[3:])\n",
    "\n",
    "    logger.info(\"Downloading data from key: %s\", key)\n",
    "\n",
    "    # path of the csv file we did uploaded in container\n",
    "    mall_fn = f\"{base_dir}/data/shopping_mall_data.csv\"\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    s3.Bucket(bucket).download_file(key, mall_fn)\n",
    "\n",
    "\n",
    "    logger.debug(\"Reading downloaded customer data.\")\n",
    "    customer_df = pd.read_csv(customer_fn)\n",
    "    os.unlink(customer_fn)\n",
    "\n",
    "    logger.debug(\"Reading downloaded sales data.\")\n",
    "    sales_df = pd.read_csv(sales_fn)\n",
    "    os.unlink(sales_fn)\n",
    "\n",
    "    logger.debug(\"Reading downloaded shooping mall data.\")\n",
    "    mall_df = pd.read_csv(mall_fn)\n",
    "    os.unlink(mall_fn)\n",
    "\n",
    "        # --- Step 1: Merge Data ---\n",
    "    # Merge Customer Data and Sales Data\n",
    "    merged_data = pd.merge(sales_df, customer_df, on=\"customer_id\", how=\"inner\")\n",
    "    # Merge with Shopping Mall Data\n",
    "    final_data = pd.merge(merged_data, mall_df, on=\"shopping_mall\", how=\"inner\")\n",
    "\n",
    "    final_data=final_data.drop([\"invoice_no\",\"customer_id\",\"invoice date\",'area (sqm)'],axis=1)\n",
    "\n",
    "\n",
    "    # Function.columns[0].[0].rmer helps us to convert those encoded functions to transformers which we can use inside sklearn pipeline\n",
    "    category_transformer = FunctionTransformer(category_encoder)\n",
    "    mall_transformer = FunctionTransformer(mall_encoder)\n",
    "    gender_transformer = FunctionTransformer(gender_encoder)\n",
    "    paymethod_transformer = FunctionTransformer(paymethod_encoder)\n",
    "    location_transformer = FunctionTransformer(location_encoder)\n",
    "    \n",
    "    # defining the numeric pipeline\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[(\"scaler\", StandardScaler())]\n",
    "    )\n",
    "    \n",
    "    # defining all categorical features\n",
    "    categorical_features = [\"category\", \"shopping_mall\", \"gender\", \"payment_method\",\"location\"]\n",
    "    \n",
    "    # creating cateorical pipeline\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"category_transformer\", category_transformer),\n",
    "            (\"mall_transformer\", mall_transformer),\n",
    "            (\"gender_transformer\", gender_transformer),\n",
    "            (\"paymethod_transformer\", paymethod_transformer),\n",
    "            (\"location_transformer\", location_transformer),\n",
    "        \n",
    "        ]\n",
    "    )\n",
    "    numeric_features = ['quantity', 'age',\n",
    "         'construction_year',\n",
    "           'store_count']\n",
    "    # we put all transformation in one place which can affect all the columns in the dataset whatever names we specified\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # splitting the data set\n",
    "    train, test = train_test_split(final_data, test_size=0.20, shuffle=True)\n",
    "    # split the dataset train and validation\n",
    "    train, validation = train_test_split(train, test_size=0.15, shuffle=True)\n",
    "\n",
    "    train_x=train.drop([\"price\"],axis=1)\n",
    "    train_y=train[\"price\"]\n",
    "    \n",
    "    test_x=test.drop([\"price\"],axis=1)\n",
    "    test_y=test[\"price\"]\n",
    "    \n",
    "    validation_x=validation.drop([\"price\"],axis=1)\n",
    "    validation_y=validation[\"price\"]\n",
    "    # fitting the preprocessor\n",
    "    all_columns_names=final_data.drop([\"price\"],axis=1).columns\n",
    "    featurized_model = preprocess.fit(train_x)\n",
    "    \n",
    "    # convert transformed numpy array as dataframe in one place\n",
    "    train_x = pd.DataFrame(featurized_model.transform(train_x), columns=all_columns_names)\n",
    "    train_y = train_y.to_numpy().reshape(len(train_y), 1)\n",
    "    train_X = np.array(train_x)\n",
    "    \n",
    "    # concatenate all the features\n",
    "    train = np.concatenate((train_y, train_X), axis=1)\n",
    "    # logger.info(\"__________________Train Transformation Ended__________________\")\n",
    "    ############################TRAINING TRANSFORMATION ENDED###########################\n",
    "    \n",
    "    ############################TESTING TRANSFORMATION STARTED###########################\n",
    "    # logger.info(\"__________________Test Transformation Started__________________\")\n",
    "    test_x = pd.DataFrame(featurized_model.transform(test_x), columns=all_columns_names)\n",
    "    test_y = test_y.to_numpy().reshape(len(test_y), 1)\n",
    "    test_X = np.array(test_x)\n",
    "    \n",
    "    test = np.concatenate((test_y, test_X), axis=1)\n",
    "    # logger.info(\"__________________Test Transformation Ended__________________\")\n",
    "    ############################TESTING TRANSFORMATION ENDED###########################\n",
    "    \n",
    "    ############################VALIDATION TRANSFORMATION STARTED###########################\n",
    "    # logger.info(\"__________________Validation Transformation Started__________________\")\n",
    "    validation_x = pd.DataFrame(featurized_model.transform(validation_x), columns=all_columns_names)\n",
    "    validation_y = validation_y.to_numpy().reshape(len(validation_y), 1)\n",
    "    validation_X = np.array(validation_x)\n",
    "    \n",
    "    validation = np.concatenate((validation_y, validation_X), axis=1)\n",
    "    # logger.info(\"__________________Validation Transformation Ended__________________\")\n",
    "    ############################VALIDATION TRANSFORMATION ENDED###########################\n",
    "\n",
    "    all_columns_names=all_columns_names.insert(0,\"price\")\n",
    "    # converting all the train and test and validation dataset to csv files\n",
    "    \n",
    "    train=pd.DataFrame(train,columns=all_columns_names)\n",
    "    validation=pd.DataFrame(validation,columns=all_columns_names)\n",
    "    test=pd.DataFrame(test,columns=all_columns_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392be98-8b72-4aa1-babd-bc932ac40c18",
   "metadata": {},
   "source": [
    "### SKLearn Processor instance or estimator for processing job\n",
    "creating an instance of a SKLearnProcessor processor and use that in our ProcessingStep of sagemaker workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "543c98a7-692d-444f-b2de-544ccd3cf858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's the define the processor using sklearn\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_preprocessing_framework_version=\"0.23-1\"\n",
    "\n",
    "sklearn_preprocessor=SKLearnProcessor(framework_version=sklearn_preprocessing_framework_version,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-process\",\n",
    "    role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575312c8-e8f0-4b85-9fc5-a8733e0751eb",
   "metadata": {},
   "source": [
    "Use the processor instance to construct a ProcessingStep of sagemaker workflow, along with the input and output channels, and the code that runs when the pipeline invokes pipeline execution. This is similar to a sklearn processor instance‚Äôs run() method in the Python SDK.Note the input_data parameters passed into ProcessingStep is the input data used in the step. This input data is used by the sklearn processor instance when it is run.\n",
    "\n",
    "\n",
    "the \"train_data\" and \"test_data\" named channels specified in the output configuration for the processing job\n",
    "Step Properties can be used in subsequent steps and resolve to their runtime values at execution. Specifically, this usage is called out when you define the training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9a84bae-15e2-4fda-b6e0-b4efb3f2418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #let' define the steps while processing \n",
    "# from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "# from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "# step_process = ProcessingStep(\n",
    "#     name = \"HotelProcess\",\n",
    "#     processor = sklearn_preprocessor,\n",
    "#     outputs = [\n",
    "#         ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\", destination=f\"s3://{bucket_name}preprocesseddata/train\"),\n",
    "#         ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\", destination=f\"s3://{bucket_name}preprocesseddata/test\"),\n",
    "#         ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\", destination=f\"s3://{bucket_name}preprocesseddata/validation\"),\n",
    "#         ProcessingOutput(output_name=\"model\", source=\"/opt/ml/processing/model\", destination=f\"s3://{bucket_name}sklearn-preprocessed-model/model-artifact\"),\n",
    "#     ],\n",
    "#     code = \"scripts/preprocessing_file.py\",\n",
    "#     job_arguments=[\"--input-data\", input_data]\n",
    "# )\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "# Define the processing step\n",
    "step_process = ProcessingStep(\n",
    "    name=\"Processing\",\n",
    "    processor=sklearn_preprocessor,\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\", destination=f\"s3://{bucket_name}preprocesseddata/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\", destination=f\"s3://{bucket_name}preprocesseddata/test\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\", destination=f\"s3://{bucket_name}preprocesseddata/validation\"),\n",
    "        ProcessingOutput(output_name=\"model\", source=\"/opt/ml/processing/model\", destination=f\"s3://{bucket_name}sklearn-preprocessed-model/model-artifact\"),\n",
    "    ],\n",
    "    code=\"scripts/preprocessing_file.py\",\n",
    "    job_arguments=[\"--input-customer-data\", input_customer_data,\n",
    "                   \"--input-sales-data\", input_data_sales,\n",
    "                   \"--input-mall-data\", input_data_shopping_mall]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a60ee0a-234f-4ba0-97c1-8577c7eb295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"SalesPipeLine\"\n",
    "pipeline = Pipeline(\n",
    "    name = pipeline_name,\n",
    "    parameters = [\n",
    "        processing_instance_count,\n",
    "        input_customer_data,\n",
    "        input_data_sales,\n",
    "        input_data_shopping_mall\n",
    "    ],\n",
    "    steps = [step_process]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e3a76-9780-4a9a-b835-5d97cf73ff69",
   "metadata": {},
   "source": [
    "### Examining the pipeline definition\n",
    "The JSON of the pipeline definition can be examined to confirm the pipeline is well-defined and the parameters and step properties resolve correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e55c2d64-f4cb-42cb-b158-3028664be48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceCount',\n",
       "   'Type': 'Integer',\n",
       "   'DefaultValue': 1},\n",
       "  {'Name': 'InputDatasales',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://pipeline-bucket-777/california-pipeline-case-study/sales_data.csv'},\n",
       "  {'Name': 'InputDatasmall',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://pipeline-bucket-777/california-pipeline-case-study/shopping_mall_data.csv'}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'Processing',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '720646828776.dkr.ecr.ap-south-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "     'ContainerArguments': ['--input-customer-data',\n",
       "      's3://pipeline-bucket-777/california-pipeline-case-study/customer_data.csv',\n",
       "      '--input-sales-data',\n",
       "      {'Get': 'Parameters.InputDatasales'},\n",
       "      '--input-mall-data',\n",
       "      {'Get': 'Parameters.InputDatasmall'}],\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocessing_file.py']},\n",
       "    'RoleArn': 'arn:aws:iam::730335253621:role/service-role/AmazonSageMaker-ExecutionRole-20241117T225496',\n",
       "    'ProcessingInputs': [{'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-ap-south-1-730335253621/Processing-c877c40909dfe1924cb80148add23d43/input/code/preprocessing_file.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://pipeline-bucket-777/california-pipeline-case-studypreprocesseddata/train',\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://pipeline-bucket-777/california-pipeline-case-studypreprocesseddata/test',\n",
       "        'LocalPath': '/opt/ml/processing/test',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'validation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://pipeline-bucket-777/california-pipeline-case-studypreprocesseddata/validation',\n",
       "        'LocalPath': '/opt/ml/processing/validation',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'model',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://pipeline-bucket-777/california-pipeline-case-studysklearn-preprocessed-model/model-artifact',\n",
       "        'LocalPath': '/opt/ml/processing/model',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c8b79-ff08-474c-9954-c3c5d23c945e",
   "metadata": {},
   "source": [
    "### Submit the pipeline to SageMaker and start execution\n",
    "\n",
    "Submit the pipeline definition to the Pipeline service. The Pipeline service uses the role that is passed in to create all the jobs defined in the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95db61b0-3ca9-4fe7-b6d3-e8bfa6befd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:ap-south-1:730335253621:pipeline/SalesPipeLine',\n",
       " 'ResponseMetadata': {'RequestId': '190c9d72-39b4-46af-a246-9aff348592ec',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '190c9d72-39b4-46af-a246-9aff348592ec',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '82',\n",
       "   'date': 'Tue, 19 Nov 2024 05:49:27 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc6012f9-4517-43b0-94ef-81fce4fba91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f34b5db-8e26-4ebe-aef1-f69eca6396f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0bd381-5278-472c-b4c8-bf371404e1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
